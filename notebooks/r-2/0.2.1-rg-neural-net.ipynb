{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural net with fasttext embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T04:56:27.021929Z",
     "start_time": "2021-05-02T04:56:26.219272Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rustem.galiullin/miniconda3/envs/tgcat/lib/python3.7/site-packages/ipykernel_launcher.py:16: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#standard libs\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import random\n",
    "import json\n",
    "import itertools\n",
    "from datetime import datetime as dt\n",
    "# ds libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "# custom path\n",
    "os.chdir('../..')\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T04:56:27.042578Z",
     "start_time": "2021-05-02T04:56:27.023706Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.train.data_utils import load_data, select_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T05:46:07.080785Z",
     "start_time": "2021-05-02T05:46:07.026073Z"
    },
    "code_folding": [
     18,
     28,
     89
    ]
   },
   "outputs": [],
   "source": [
    "# %%writefile src/train/neural.py\n",
    "\"\"\"\n",
    "Load fasttext weights and build a nueral net\n",
    "\"\"\"\n",
    "\n",
    "import io\n",
    "from typing import List, Dict\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "rand_true = lambda test_prob: np.random.choice([True,False], p=[test_prob, 1 - test_prob])\n",
    "\n",
    "\n",
    "def select_test(data, test_subsets=None,  test_size=0.3):\n",
    "    \"\"\" Return True for test set and False for train \"\"\"\n",
    "    if test_subsets is None:\n",
    "        test_subsets = data['subset'].unique().tolist()\n",
    "    is_test = data.apply(lambda x: rand_true(test_size) if x['subset'] in test_subsets else False,\n",
    "              axis=1)\n",
    "    return is_test\n",
    "\n",
    "\n",
    "\n",
    "def load_vectors(fname,):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    i = 0\n",
    "    for line in tqdm(fin, desc='lines'):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "    fin.close()\n",
    "    return data, d\n",
    "\n",
    "\n",
    "def weights_to_tensor(topic_weights: Dict[str, float], classes: List[str]) -> torch.Tensor:\n",
    "    \"\"\" convert dict weights to a tensor \"\"\"\n",
    "    weight_vec = torch.zeros(len(classes))\n",
    "    for t,w in topic_weights.items():\n",
    "        index = classes.index(t)\n",
    "        weight_vec[index] = w\n",
    "    return weight_vec\n",
    "\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding network to convert tokenized text to word vectors\n",
    "    \"\"\"\n",
    "    def __init__(self, vectors: Dict[str, List[float]], dim: int):\n",
    "        super().__init__()\n",
    "        self.vocab = {w:i for i,w in enumerate(list(vectors.keys()))}\n",
    "        self.dim = dim\n",
    "        self.id2word = {i:w for i,w in self.vocab.items()}\n",
    "        self.embeddings = torch.zeros(len(self.vocab), dim, requires_grad=False)\n",
    "        for w,i in self.vocab.items():\n",
    "            self.embeddings[i] = torch.tensor(vectors[w])\n",
    "            \n",
    "            \n",
    "    \n",
    "    def get_doc_vectors(self, doc: List[str]) -> List[torch.Tensor]:\n",
    "        \"\"\" convert a list of tokens to a list of word vectors, if not present skip \"\"\"\n",
    "        res = [self.embeddings[self.vocab[token]] for token in doc if token in self.vocab.keys()]\n",
    "        if len(res) == 0:\n",
    "            res = [torch.zeros(self.dim)]\n",
    "        return res\n",
    "            \n",
    "    \n",
    "    def forward(self, documents: List[List[str]]) -> List[List[torch.Tensor]]:\n",
    "        \"\"\" get word vectors for a batch of documents \"\"\"\n",
    "        res = [self.get_doc_vectors(doc) for doc in documents]\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    def get_extremes(self, doc_vecs: List[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\" calculate mean,max,min and sum along each dimension and concat \"\"\"\n",
    "        if len(doc_vecs) == 0:\n",
    "            return torch.zeros(self.dim * 4)\n",
    "        stacked = torch.stack(doc_vecs, dim=0)\n",
    "        t_max,_ = stacked.max(dim=0)\n",
    "        t_min,_ = stacked.min(dim=0)\n",
    "        t_mean = stacked.mean(0)\n",
    "        t_sum = stacked.sum(0)\n",
    "        concat = torch.cat([t_max, t_min, t_mean, t_sum],)\n",
    "        return concat\n",
    "    \n",
    "    \n",
    "    def get_batch_extremes(self, batch:  List[List[torch.Tensor]],) -> torch.Tensor:\n",
    "        \"\"\" get extremes for each document and return as one batch tensor \"\"\"\n",
    "        batch_extremes = [self.get_extremes(doc) for doc in batch]\n",
    "        batch_extremes = torch.stack(batch_extremes)\n",
    "        return batch_extremes\n",
    "    \n",
    "    \n",
    "def list2pairs(list_obj: List[int]) -> List[List[int]]:\n",
    "    \"\"\" convert list of neural units to pairs for input and output neurons \"\"\"\n",
    "    if len(list_obj) < 2:\n",
    "#         num = list_obj[0]\n",
    "#         return [[num, num]]\n",
    "#     elif len(list_obj) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        inps = list_obj[:-1]\n",
    "        outs = list_obj[1:]\n",
    "        pairs = [[i,o] for i,o in zip(inps, outs)]\n",
    "        return pairs\n",
    "    \n",
    "    \n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, input_features: int, dense_units: List[int], num_classes: int, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        linear_neurons = list2pairs([input_features] + dense_units)\n",
    "        self.hidden = nn.Sequential(*[\n",
    "            self.linear_block(inp, out)\n",
    "         for inp,out in linear_neurons\n",
    "        ])\n",
    "        self.clf = nn.Linear(linear_neurons[-1][1] if len(linear_neurons) > 0 else input_features, num_classes)\n",
    "        \n",
    "\n",
    "    def linear_block(self, in_units, out_units ):\n",
    "        block = nn.Sequential(\n",
    "            nn.Linear(in_units, out_units),\n",
    "            nn.BatchNorm1d(out_units),\n",
    "            nn.LeakyReLU(0.02),\n",
    "            nn.Dropout(self.dropout),\n",
    "            )\n",
    "        return block\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.clf(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class TopicClassifier(nn.Module):\n",
    "    \"\"\" hold together embeddings and neural net \"\"\"\n",
    "    def __init__(self, embedding_net, output_net):\n",
    "        super().__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "        self.output_net = output_net\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        self.embedding_net.eval()\n",
    "        self.output_net.train()\n",
    "        \n",
    "        \n",
    "    def eval(self):\n",
    "        self.embedding_net.eval()\n",
    "        self.output_net.eval()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            vectors = self.embedding_net(x)\n",
    "            vectors = self.embedding_net.get_batch_extremes(vectors)\n",
    "        probs = self.output_net(vectors)\n",
    "        return probs\n",
    "        \n",
    "\n",
    "generate_vecs = lambda dim: {w:[random.random() for i in range(dim)] for w in list('asdfqwerty')}\n",
    "def generate_inputs():\n",
    "    return [list('asd'), list('asdfg'), list('qwerpoi'), list('zcvv')]\n",
    "        \n",
    "def test_embeddings():\n",
    "    dim_size = 10\n",
    "    vecs = generate_vecs(dim_size)\n",
    "    # net\n",
    "    emb_net = EmbeddingNet(vecs, dim_size)\n",
    "    assert emb_net.embeddings.size() == (len(vecs), dim_size)\n",
    "    # doc vectors\n",
    "    doc = list('qwzxdf')\n",
    "    doc_vecs = emb_net.get_doc_vectors(doc)\n",
    "    assert len(doc_vecs) == 4\n",
    "    assert doc_vecs[1].size() == (dim_size,)\n",
    "    # batch vectors\n",
    "    docs = generate_inputs()\n",
    "    batch_vecs = emb_net(docs)\n",
    "    assert len(batch_vecs) == len(docs)\n",
    "    assert len(batch_vecs[0]) == 3\n",
    "    assert len(batch_vecs[3]) == 1\n",
    "    assert batch_vecs[1][1].size() == (dim_size,)\n",
    "    # extremes\n",
    "    extremes = emb_net.get_extremes(batch_vecs[2])\n",
    "    assert extremes.size() == (dim_size * 4, )\n",
    "    assert all(extremes[:dim_size] > extremes[dim_size:dim_size*2])\n",
    "    assert all(extremes[dim_size*3:dim_size*4] > extremes[dim_size*2:dim_size*3])\n",
    "    batch_extremes = emb_net.get_batch_extremes(batch_vecs)\n",
    "    assert batch_extremes.size() == (len(docs), dim_size * 4)\n",
    "    return emb_net\n",
    "\n",
    "\n",
    "def test_dense():\n",
    "    params = dict(input_features = 10, dense_units=[10,5], num_classes=4)\n",
    "    dense = DenseNet(**params)\n",
    "    assert len(dense.hidden) == 2\n",
    "    bs = 6\n",
    "    dummy = torch.randn(bs, params['input_features'])\n",
    "    out = dense(dummy)\n",
    "    assert out.size() == (bs, params['num_classes'])\n",
    "    return dense\n",
    "\n",
    "\n",
    "def test_clf():\n",
    "    dim = 21\n",
    "    emb_params = {'vectors': generate_vecs(dim), 'dim': dim}\n",
    "    emb_net = EmbeddingNet(**emb_params)\n",
    "    dense_params = dict(input_features = dim * 4, dense_units=[13,11], num_classes=9)\n",
    "    dense_net = DenseNet(**dense_params)\n",
    "    clf = TopicClassifier(emb_net, dense_net)\n",
    "    docs = generate_inputs()\n",
    "    out = clf(docs)\n",
    "    assert out.size() == (len(docs), dense_params['num_classes'])\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T05:46:08.239308Z",
     "start_time": "2021-05-02T05:46:08.212755Z"
    }
   },
   "outputs": [],
   "source": [
    "e_net = test_embeddings()\n",
    "\n",
    "d_net = test_dense()\n",
    "\n",
    "\n",
    "c_net = test_clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T04:56:28.673340Z",
     "start_time": "2021-05-02T04:56:28.656347Z"
    }
   },
   "outputs": [],
   "source": [
    "FILE = './data/interim/train_data.csv'\n",
    "TEST_SUBSETS = ['r-1', 'r-2']\n",
    "TEST_SIZE = 0.33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text and topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T04:56:39.324934Z",
     "start_time": "2021-05-02T04:56:35.853925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33192 entries, 0 to 33191\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   title         33192 non-null  object \n",
      " 1   description   33192 non-null  object \n",
      " 2   recent_posts  33192 non-null  object \n",
      " 3   lang_code     33192 non-null  object \n",
      " 4   id            1447 non-null   float64\n",
      " 5   category      33192 non-null  object \n",
      " 6   subset        33192 non-null  object \n",
      " 7   text          33192 non-null  object \n",
      "dtypes: float64(1), object(7)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data = load_data(FILE)\n",
    "\n",
    "data['is_test'] = select_test(data, TEST_SUBSETS, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T04:56:39.362651Z",
     "start_time": "2021-05-02T04:56:39.326702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">is_test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang_code</th>\n",
       "      <th>subset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ar</th>\n",
       "      <th>r-2</th>\n",
       "      <td>0.342105</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">en</th>\n",
       "      <th>chan</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r-1</th>\n",
       "      <td>0.330247</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r-2</th>\n",
       "      <td>0.340659</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tg</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>14775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fa</th>\n",
       "      <th>r-2</th>\n",
       "      <td>0.414365</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">ru</th>\n",
       "      <th>chan</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r-1</th>\n",
       "      <td>0.335150</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r-2</th>\n",
       "      <td>0.330097</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tg</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>16726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uz</th>\n",
       "      <th>r-2</th>\n",
       "      <td>0.350785</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   is_test       \n",
       "                      mean  count\n",
       "lang_code subset                 \n",
       "ar        r-2     0.342105    190\n",
       "en        chan    0.000000     42\n",
       "          r-1     0.330247    324\n",
       "          r-2     0.340659     91\n",
       "          tg      0.000000  14775\n",
       "fa        r-2     0.414365    181\n",
       "ru        chan    0.000000    202\n",
       "          r-1     0.335150    367\n",
       "          r-2     0.330097    103\n",
       "          tg      0.000000  16726\n",
       "uz        r-2     0.350785    191"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(['lang_code', 'subset']).agg(dict(is_test=['mean','count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T04:56:39.468272Z",
     "start_time": "2021-05-02T04:56:39.364292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1691, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.query(\"subset != 'tg'\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T04:56:39.489960Z",
     "start_time": "2021-05-02T04:56:39.470047Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LANGS = data['lang_code'].unique().tolist()\n",
    "\n",
    "CLASSES = list(set([\n",
    "            c.strip() for cat in data['category'].tolist() \n",
    "            for c in cat.keys() \n",
    "        ]))\n",
    "\n",
    "\n",
    "len(CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T04:56:43.201559Z",
     "start_time": "2021-05-02T04:56:43.167658Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from src.train.text_utils import tokenize_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T04:56:43.407590Z",
     "start_time": "2021-05-02T04:56:43.379641Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextData(Dataset):\n",
    "    ''' tokenize text and convert weight dicts to vectors '''\n",
    "    def __init__(self, data, lang_code, classes, subsets=None):\n",
    "        super().__init__()\n",
    "        self.data = data.query(f'lang_code == \"{lang_code}\"')\n",
    "        if subsets:\n",
    "            self.data = self.data.query(f\"subset == {subsets}\")\n",
    "        self.classes = list(set([\n",
    "            c.strip() for cat in self.data['category'].tolist() \n",
    "            for c in cat.keys() \n",
    "        ]))\n",
    "        self.classes = classes\n",
    " \n",
    "        \n",
    "    def __len__(self, ):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        tokens = tokenize_text(row['text'])\n",
    "        cat = {c.strip():w for c,w in row['category'].items()}\n",
    "        weights = weights_to_tensor(cat, self.classes)\n",
    "        return tokens, weights\n",
    "    \n",
    "    \n",
    "    \n",
    "def create_loaders(data, lang_code, classes, batch_size=16, subsets=None):\n",
    "    train_data = data.loc[~data['is_test']]\n",
    "    test_data = data.loc[data['is_test']]\n",
    "    train_set = TextData(train_data.query('is_test == False'), lang_code, classes, subsets=subsets)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    test_set = TextData(test_data.query('is_test == True'), lang_code, classes, subsets=subsets)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    x = [b[0] for b in batch]\n",
    "    y = torch.stack([b[1] for b in batch], axis=0)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T04:56:45.273552Z",
     "start_time": "2021-05-02T04:56:45.247680Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data = TextData(data, 'ru', subsets=['r-2', 'r-1'], classes=CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T04:58:49.062460Z",
     "start_time": "2021-05-02T04:58:49.036650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T04:58:34.439014Z",
     "start_time": "2021-05-02T04:58:34.410031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([53])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T04:58:55.550309Z",
     "start_time": "2021-05-02T04:58:55.529608Z"
    }
   },
   "outputs": [],
   "source": [
    "t,w = text_data[random.randint(0, len(text_data)-1)]\n",
    "\n",
    "\n",
    "assert isinstance(t, list)\n",
    "if len(t) > 0:\n",
    "    assert isinstance(t[-1], str)\n",
    "assert isinstance(w, torch.Tensor)\n",
    "assert w.size() == (len(CLASSES),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T04:59:59.121530Z",
     "start_time": "2021-05-02T04:59:59.073157Z"
    }
   },
   "outputs": [],
   "source": [
    "bs = 3\n",
    "l,_ = create_loaders(data, 'ru', classes=CLASSES, batch_size=bs,)\n",
    "\n",
    "for x,y in l:\n",
    "    break\n",
    "    \n",
    "    \n",
    "assert len(x) == bs\n",
    "assert y.size() == (bs, len(CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T06:07:26.795431Z",
     "start_time": "2021-05-02T06:07:26.715396Z"
    }
   },
   "outputs": [],
   "source": [
    "SIZE = \"100k\"\n",
    "DIM = 300\n",
    "\n",
    "VECTORS = \"models/external/word_vectors/{size}.cc.{lang_code}.{dim}.vec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T06:09:49.169939Z",
     "start_time": "2021-05-02T06:09:49.137323Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T06:09:49.405693Z",
     "start_time": "2021-05-02T06:09:49.372247Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_mae_score(true, predicted):\n",
    "    \"\"\" run softmax over predictions and get mae score \"\"\"\n",
    "    probs = F.softmax(predicted,1)\n",
    "    mae = F.l1_loss(probs, true, reduction='sum') / true.size(0)\n",
    "    return loss2score(mae.item())\n",
    "\n",
    "\n",
    "loss2score = lambda loss: 1/(1+loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T06:24:17.049199Z",
     "start_time": "2021-05-02T06:24:10.852701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8d235a59294be4b94adab564ca7125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lines: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lang_code = 'ru'\n",
    "\n",
    "\n",
    "word_vectors,_ = load_vectors(VECTORS.format(size=SIZE, lang_code=lang_code, dim=DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T06:24:17.822941Z",
     "start_time": "2021-05-02T06:24:17.798344Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 128\n",
    "DENSE_UNITS = [128,]\n",
    "\n",
    "ETA = 0.01\n",
    "DROPOUT = 0.25\n",
    "\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T06:24:20.022539Z",
     "start_time": "2021-05-02T06:24:18.121049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopicClassifier(\n",
      "  (embedding_net): EmbeddingNet()\n",
      "  (output_net): DenseNet(\n",
      "    (hidden): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=1200, out_features=128, bias=True)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.02)\n",
      "        (3): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (clf): Linear(in_features=128, out_features=53, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_loader,test_loader = create_loaders(data, lang_code, classes=CLASSES, batch_size=BATCH_SIZE)\n",
    "\n",
    "embed = EmbeddingNet(word_vectors, DIM)\n",
    "\n",
    "dense = DenseNet(DIM * 4, DENSE_UNITS, len(CLASSES), DROPOUT)\n",
    "\n",
    "clf = TopicClassifier(embed, dense)\n",
    "\n",
    "print(clf)\n",
    "\n",
    "criterion = nn.L1Loss(reduction='sum')\n",
    "\n",
    "optimizer = optim.Adam(dense.parameters(), lr=ETA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T06:35:43.870712Z",
     "start_time": "2021-05-02T06:32:28.458609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train 1.2495, eval 1.2892\n",
      "Epoch 2: train 1.2494, eval 1.2513\n",
      "Epoch 3: train 1.2331, eval 1.2403\n",
      "Epoch 4: train 1.2150, eval 1.2203\n",
      "Epoch 5: train 1.2040, eval 1.2400\n",
      "Epoch 6: train 1.2045, eval 1.2343\n",
      "Epoch 7: train 1.2015, eval 1.2221\n",
      "Epoch 8: train 1.1898, eval 1.2367\n",
      "Epoch 9: train 1.1949, eval 1.2482\n",
      "Epoch 10: train 1.2124, eval 1.2447\n",
      "Epoch 11: train 1.2047, eval 1.2228\n",
      "Epoch 12: train 1.1883, eval 1.1897\n",
      "Epoch 13: train 1.1754, eval 1.1951\n",
      "Epoch 14: train 1.1681, eval 1.2199\n",
      "Epoch 15: train 1.1890, eval 1.2024\n",
      "Epoch 16: train 1.1958, eval 1.1979\n",
      "Epoch 17: train 1.1823, eval 1.1641\n",
      "Epoch 18: train 1.1589, eval 1.1668\n",
      "Epoch 19: train 1.1648, eval 1.1671\n",
      "Epoch 20: train 1.1723, eval 1.1531\n",
      "Epoch 21: train 1.1479, eval 1.1704\n",
      "Epoch 22: train 1.1560, eval 1.1703\n",
      "Epoch 23: train 1.1599, eval 1.1668\n",
      "Epoch 24: train 1.1538, eval 1.1589\n",
      "Epoch 25: train 1.1450, eval 1.1529\n",
      "Epoch 26: train 1.1419, eval 1.1588\n",
      "Epoch 27: train 1.1436, eval 1.1687\n",
      "Epoch 28: train 1.1471, eval 1.1816\n",
      "Epoch 29: train 1.1623, eval 1.1553\n",
      "Epoch 30: train 1.1450, eval 1.1704\n",
      "Scores: train 0.47, eval 0.46\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCHS):\n",
    "    # train\n",
    "    clf.train()\n",
    "    train_loss = 0.\n",
    "    train_size = 0.\n",
    "    for x,y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = clf(x)\n",
    "        loss = criterion(out, y)\n",
    "        train_loss += loss.item()\n",
    "        train_size += y.size(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= train_size\n",
    "    # eval\n",
    "    clf.eval()\n",
    "    test_loss = 0.\n",
    "    test_size = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in test_loader:\n",
    "            out = clf(x)\n",
    "            loss = criterion(out, y)\n",
    "            test_loss += loss.item()\n",
    "            test_size += y.size(0)\n",
    "    test_loss /= test_size\n",
    "    print(f\"Epoch {i+1}: train {train_loss:.4f}, eval {test_loss:.4f}\")\n",
    "    \n",
    "print(f\"Scores: train {loss2score(train_loss):.2f}, eval {loss2score(test_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgcat",
   "language": "python",
   "name": "tgcat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "notify_time": "10",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
