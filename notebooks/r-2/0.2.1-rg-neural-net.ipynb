{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural net with fasttext embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T04:56:27.021929Z",
     "start_time": "2021-05-02T04:56:26.219272Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rustem.galiullin/miniconda3/envs/tgcat/lib/python3.7/site-packages/ipykernel_launcher.py:16: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#standard libs\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import random\n",
    "import json\n",
    "import itertools\n",
    "from datetime import datetime as dt\n",
    "# ds libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "# custom path\n",
    "os.chdir('../..')\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T04:56:27.042578Z",
     "start_time": "2021-05-02T04:56:27.023706Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.train.data_utils import load_data, select_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T10:27:44.785291Z",
     "start_time": "2021-05-02T10:27:44.738713Z"
    },
    "code_folding": [
     18,
     28,
     169,
     197
    ]
   },
   "outputs": [],
   "source": [
    "# %%writefile src/train/neural.py\n",
    "\"\"\"\n",
    "Load fasttext weights and build a nueral net\n",
    "\"\"\"\n",
    "\n",
    "import io\n",
    "from typing import List, Dict\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "rand_true = lambda test_prob: np.random.choice([True,False], p=[test_prob, 1 - test_prob])\n",
    "\n",
    "\n",
    "def select_test(data, test_subsets=None,  test_size=0.3):\n",
    "    \"\"\" Return True for test set and False for train \"\"\"\n",
    "    if test_subsets is None:\n",
    "        test_subsets = data['subset'].unique().tolist()\n",
    "    is_test = data.apply(lambda x: rand_true(test_size) if x['subset'] in test_subsets else False,\n",
    "              axis=1)\n",
    "    return is_test\n",
    "\n",
    "\n",
    "\n",
    "def load_vectors(fname,):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    i = 0\n",
    "    for line in tqdm(fin, desc='lines'):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "    fin.close()\n",
    "    return data, d\n",
    "\n",
    "\n",
    "def weights_to_tensor(topic_weights: Dict[str, float], classes: List[str]) -> torch.Tensor:\n",
    "    \"\"\" convert dict weights to a tensor \"\"\"\n",
    "    weight_vec = torch.zeros(len(classes))\n",
    "    for t,w in topic_weights.items():\n",
    "        index = classes.index(t)\n",
    "        weight_vec[index] = w\n",
    "    return weight_vec\n",
    "\n",
    "\n",
    "def list2pairs(list_obj: List[int]) -> List[List[int]]:\n",
    "    \"\"\" convert list of neural units to pairs for input and output neurons \"\"\"\n",
    "    if len(list_obj) < 2:\n",
    "        return []\n",
    "    else:\n",
    "        inps = list_obj[:-1]\n",
    "        outs = list_obj[1:]\n",
    "        pairs = [[i,o] for i,o in zip(inps, outs)]\n",
    "        return pairs\n",
    "\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding network to convert tokenized text to word vectors\n",
    "    \"\"\"\n",
    "    def __init__(self, vectors: Dict[str, List[float]], dim: int):\n",
    "        super().__init__()\n",
    "        self.vocab = {w:i for i,w in enumerate(list(vectors.keys()))}\n",
    "        self.dim = dim\n",
    "        self.id2word = {i:w for i,w in self.vocab.items()}\n",
    "        self.embeddings = torch.zeros(len(self.vocab), dim, requires_grad=False)\n",
    "        for w,i in self.vocab.items():\n",
    "            self.embeddings[i] = torch.tensor(vectors[w])\n",
    "            \n",
    "            \n",
    "    \n",
    "    def get_doc_vectors(self, doc: List[str]) -> List[torch.Tensor]:\n",
    "        \"\"\" convert a list of tokens to a list of word vectors, if not present skip \"\"\"\n",
    "        res = [self.embeddings[self.vocab[token]] for token in doc if token in self.vocab.keys()]\n",
    "        if len(res) == 0:\n",
    "            res = [torch.zeros(self.dim)]\n",
    "        return res\n",
    "            \n",
    "    \n",
    "    def forward(self, documents: List[List[str]]) -> List[List[torch.Tensor]]:\n",
    "        \"\"\" get word vectors for a batch of documents \"\"\"\n",
    "        res = [self.get_doc_vectors(doc) for doc in documents]\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    def get_extremes(self, doc_vecs: List[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\" calculate mean,max,min and sum along each dimension and concat \"\"\"\n",
    "        if len(doc_vecs) == 0:\n",
    "            return torch.zeros(self.dim * 4)\n",
    "        stacked = torch.stack(doc_vecs, dim=0)\n",
    "        t_max,_ = stacked.max(dim=0)\n",
    "        t_min,_ = stacked.min(dim=0)\n",
    "        t_mean = stacked.mean(0)\n",
    "        t_sum = stacked.sum(0)\n",
    "        concat = torch.cat([t_max, t_min, t_mean, t_sum],)\n",
    "        return concat\n",
    "    \n",
    "    \n",
    "    def get_batch_extremes(self, batch:  List[List[torch.Tensor]],) -> torch.Tensor:\n",
    "        \"\"\" get extremes for each document and return as one batch tensor \"\"\"\n",
    "        batch_extremes = [self.get_extremes(doc) for doc in batch]\n",
    "        batch_extremes = torch.stack(batch_extremes)\n",
    "        return batch_extremes\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, input_features: int, dense_units: List[int], num_classes: int, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        linear_neurons = list2pairs([input_features] + dense_units)\n",
    "        self.hidden = nn.Sequential(*[\n",
    "            self.linear_block(inp, out)\n",
    "         for inp,out in linear_neurons\n",
    "        ])\n",
    "        self.clf = nn.Linear(linear_neurons[-1][1] if len(linear_neurons) > 0 else input_features, num_classes)\n",
    "        \n",
    "\n",
    "    def linear_block(self, in_units, out_units ):\n",
    "        block = nn.Sequential(\n",
    "            nn.Linear(in_units, out_units),\n",
    "            nn.BatchNorm1d(out_units),\n",
    "            nn.LeakyReLU(0.02),\n",
    "            nn.Dropout(self.dropout),\n",
    "            )\n",
    "        return block\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.clf(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class TopicClassifier(nn.Module):\n",
    "    \"\"\" hold together embeddings and neural net \"\"\"\n",
    "    def __init__(self, embedding_net, output_net, is_rnn=False):\n",
    "        super().__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "        self.output_net = output_net\n",
    "        self.is_rnn = is_rnn\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        self.embedding_net.eval()\n",
    "        self.output_net.train()\n",
    "        \n",
    "        \n",
    "    def eval(self):\n",
    "        self.embedding_net.eval()\n",
    "        self.output_net.eval()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            vectors = self.embedding_net(x)\n",
    "            if not self.is_rnn:\n",
    "                vectors = self.embedding_net.get_batch_extremes(vectors)\n",
    "        probs = self.output_net(vectors)\n",
    "        return probs\n",
    "        \n",
    "\n",
    "generate_vecs = lambda dim: {w:[random.random() for i in range(dim)] for w in list('asdfqwerty')}\n",
    "def generate_inputs():\n",
    "    return [list('asd'), list('asdfg'), list('qwerpoi'), list('zcvv')]\n",
    "        \n",
    "def test_embeddings():\n",
    "    dim_size = 10\n",
    "    vecs = generate_vecs(dim_size)\n",
    "    # net\n",
    "    emb_net = EmbeddingNet(vecs, dim_size)\n",
    "    assert emb_net.embeddings.size() == (len(vecs), dim_size)\n",
    "    # doc vectors\n",
    "    doc = list('qwzxdf')\n",
    "    doc_vecs = emb_net.get_doc_vectors(doc)\n",
    "    assert len(doc_vecs) == 4\n",
    "    assert doc_vecs[1].size() == (dim_size,)\n",
    "    # batch vectors\n",
    "    docs = generate_inputs()\n",
    "    batch_vecs = emb_net(docs)\n",
    "    assert len(batch_vecs) == len(docs)\n",
    "    assert len(batch_vecs[0]) == 3\n",
    "    assert len(batch_vecs[3]) == 1\n",
    "    assert batch_vecs[1][1].size() == (dim_size,)\n",
    "    # extremes\n",
    "    extremes = emb_net.get_extremes(batch_vecs[2])\n",
    "    assert extremes.size() == (dim_size * 4, )\n",
    "    assert all(extremes[:dim_size] > extremes[dim_size:dim_size*2])\n",
    "    assert all(extremes[dim_size*3:dim_size*4] > extremes[dim_size*2:dim_size*3])\n",
    "    batch_extremes = emb_net.get_batch_extremes(batch_vecs)\n",
    "    assert batch_extremes.size() == (len(docs), dim_size * 4)\n",
    "    return emb_net\n",
    "\n",
    "\n",
    "def test_dense():\n",
    "    params = dict(input_features = 10, dense_units=[10,5], num_classes=4)\n",
    "    dense = DenseNet(**params)\n",
    "    assert len(dense.hidden) == 2\n",
    "    bs = 6\n",
    "    dummy = torch.randn(bs, params['input_features'])\n",
    "    out = dense(dummy)\n",
    "    assert out.size() == (bs, params['num_classes'])\n",
    "    return dense\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def test_clf():\n",
    "    dim = 21\n",
    "    emb_params = {'vectors': generate_vecs(dim), 'dim': dim}\n",
    "    emb_net = EmbeddingNet(**emb_params)\n",
    "    dense_params = dict(input_features = dim * 4, dense_units=[13,11], num_classes=9)\n",
    "    dense_net = DenseNet(**dense_params)\n",
    "    clf = TopicClassifier(emb_net, dense_net)\n",
    "    docs = generate_inputs()\n",
    "    out = clf(docs)\n",
    "    assert out.size() == (len(docs), dense_params['num_classes'])\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:18:41.046565Z",
     "start_time": "2021-05-02T14:18:41.021000Z"
    }
   },
   "outputs": [],
   "source": [
    "class RecurrentNet(nn.Module):\n",
    "    def __init__(self, input_features: int, num_classes: int,\n",
    "                 hidden_units: int, num_layers: int=1,\n",
    "                 bidirect: bool=False, dropout=0):\n",
    "        super().__init__()\n",
    "        self.recurrent_layer = nn.GRU(input_features, hidden_units, num_layers, \n",
    "                                      dropout=dropout, bidirectional=bidirect,\n",
    "                                     batch_first=True)\n",
    "        out_size = hidden_units * 2 if bidirect else hidden_units * 1\n",
    "        self.clf = nn.Linear(out_size, num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x: List[List[torch.Tensor]]) -> torch.Tensor:\n",
    "        stacked = [torch.stack(embs, 0) for embs in x]\n",
    "        padded = nn.utils.rnn.pad_sequence(stacked, batch_first=False)\n",
    "        out,hidden = self.recurrent_layer(padded)\n",
    "        out = out.transpose(1,0)\n",
    "        out = out.mean(1)\n",
    "#         out = torch.cat([out.mean(1), out[:,-1,:]], dim=1)\n",
    "        out = self.clf(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:17:22.626978Z",
     "start_time": "2021-05-02T14:17:22.573226Z"
    }
   },
   "outputs": [],
   "source": [
    "e_net = test_embeddings()\n",
    "\n",
    "d_net = test_dense()\n",
    "\n",
    "\n",
    "c_net = test_clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:17:23.145699Z",
     "start_time": "2021-05-02T14:17:23.122588Z"
    }
   },
   "outputs": [],
   "source": [
    "FILE = './data/interim/train_data.csv'\n",
    "TEST_SUBSETS = ['r-1', 'r-2']\n",
    "TEST_SIZE = 0.33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text and topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:17:31.000688Z",
     "start_time": "2021-05-02T14:17:25.563240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33236 entries, 0 to 33235\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   title         33236 non-null  object \n",
      " 1   description   33236 non-null  object \n",
      " 2   recent_posts  33236 non-null  object \n",
      " 3   lang_code     33236 non-null  object \n",
      " 4   id            1491 non-null   float64\n",
      " 5   category      33236 non-null  object \n",
      " 6   subset        33236 non-null  object \n",
      " 7   text          33236 non-null  object \n",
      "dtypes: float64(1), object(7)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data = load_data(FILE)\n",
    "\n",
    "data['is_test'] = select_test(data, TEST_SUBSETS, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:17:31.041678Z",
     "start_time": "2021-05-02T14:17:31.002058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">is_test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang_code</th>\n",
       "      <th>subset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ar</th>\n",
       "      <th>r-2</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">en</th>\n",
       "      <th>chan</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r-1</th>\n",
       "      <td>0.336420</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r-2</th>\n",
       "      <td>0.351648</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tg</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>14775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fa</th>\n",
       "      <th>r-2</th>\n",
       "      <td>0.317949</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">ru</th>\n",
       "      <th>chan</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r-1</th>\n",
       "      <td>0.316076</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r-2</th>\n",
       "      <td>0.281553</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tg</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>16726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uz</th>\n",
       "      <th>r-2</th>\n",
       "      <td>0.285024</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   is_test       \n",
       "                      mean  count\n",
       "lang_code subset                 \n",
       "ar        r-2     0.352941    204\n",
       "en        chan    0.000000     42\n",
       "          r-1     0.336420    324\n",
       "          r-2     0.351648     91\n",
       "          tg      0.000000  14775\n",
       "fa        r-2     0.317949    195\n",
       "ru        chan    0.000000    202\n",
       "          r-1     0.316076    367\n",
       "          r-2     0.281553    103\n",
       "          tg      0.000000  16726\n",
       "uz        r-2     0.285024    207"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(['lang_code', 'subset']).agg(dict(is_test=['mean','count']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:17:31.177183Z",
     "start_time": "2021-05-02T14:17:31.043345Z"
    }
   },
   "source": [
    "data = data.query(\"subset != 'tg'\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:18:04.008294Z",
     "start_time": "2021-05-02T14:18:03.979716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LANGS = data['lang_code'].unique().tolist()\n",
    "\n",
    "CLASSES = list(set([\n",
    "            c.strip() for cat in data['category'].tolist() \n",
    "            for c in cat.keys() \n",
    "        ]))\n",
    "\n",
    "\n",
    "len(CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:18:05.238650Z",
     "start_time": "2021-05-02T14:18:05.216636Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from src.train.text_utils import tokenize_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:18:05.401221Z",
     "start_time": "2021-05-02T14:18:05.373479Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextData(Dataset):\n",
    "    ''' tokenize text and convert weight dicts to vectors '''\n",
    "    def __init__(self, data, lang_code, classes, subsets=None):\n",
    "        super().__init__()\n",
    "        self.data = data.query(f'lang_code == \"{lang_code}\"')\n",
    "        if subsets:\n",
    "            self.data = self.data.query(f\"subset == {subsets}\")\n",
    "        self.classes = list(set([\n",
    "            c.strip() for cat in self.data['category'].tolist() \n",
    "            for c in cat.keys() \n",
    "        ]))\n",
    "        self.classes = classes\n",
    " \n",
    "        \n",
    "    def __len__(self, ):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        tokens = tokenize_text(row['text'])\n",
    "        cat = {c.strip():w for c,w in row['category'].items()}\n",
    "        weights = weights_to_tensor(cat, self.classes)\n",
    "        return tokens, weights\n",
    "    \n",
    "    \n",
    "    \n",
    "def create_loaders(data, lang_code, classes, batch_size=16, subsets=None):\n",
    "    train_data = data.loc[~data['is_test']]\n",
    "    test_data = data.loc[data['is_test']]\n",
    "    train_set = TextData(train_data.query('is_test == False'), lang_code, classes, subsets=subsets)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    test_set = TextData(test_data.query('is_test == True'), lang_code, classes, subsets=subsets)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    x = [b[0] for b in batch]\n",
    "    y = torch.stack([b[1] for b in batch], axis=0)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:18:07.187077Z",
     "start_time": "2021-05-02T14:18:07.160200Z"
    }
   },
   "outputs": [],
   "source": [
    "text_data = TextData(data, 'ru', subsets=['r-2', 'r-1'], classes=CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:18:08.395132Z",
     "start_time": "2021-05-02T14:18:08.362594Z"
    }
   },
   "outputs": [],
   "source": [
    "t,w = text_data[random.randint(0, len(text_data)-1)]\n",
    "\n",
    "\n",
    "assert isinstance(t, list)\n",
    "if len(t) > 0:\n",
    "    assert isinstance(t[-1], str)\n",
    "assert isinstance(w, torch.Tensor)\n",
    "assert w.size() == (len(CLASSES),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:18:09.061048Z",
     "start_time": "2021-05-02T14:18:09.012528Z"
    }
   },
   "outputs": [],
   "source": [
    "bs = 3\n",
    "l,_ = create_loaders(data, 'ru', classes=CLASSES, batch_size=bs,)\n",
    "\n",
    "for x,y in l:\n",
    "    break\n",
    "    \n",
    "    \n",
    "assert len(x) == bs\n",
    "assert y.size() == (bs, len(CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:18:10.340501Z",
     "start_time": "2021-05-02T14:18:10.318890Z"
    }
   },
   "outputs": [],
   "source": [
    "SIZE = \"100k\"\n",
    "DIM = 300\n",
    "\n",
    "VECTORS = \"models/external/word_vectors/{size}.cc.{lang_code}.{dim}.vec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:18:10.968479Z",
     "start_time": "2021-05-02T14:18:10.944526Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:18:11.177092Z",
     "start_time": "2021-05-02T14:18:11.152884Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_mae_score(true, predicted):\n",
    "    \"\"\" run softmax over predictions and get mae score \"\"\"\n",
    "    probs = F.softmax(predicted,1)\n",
    "    mae = F.l1_loss(probs, true, reduction='sum') / true.size(0)\n",
    "    return loss2score(mae.item())\n",
    "\n",
    "\n",
    "loss2score = lambda loss: 1/(1+loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:18:54.708499Z",
     "start_time": "2021-05-02T14:18:47.429119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880dc17cdc934adfbf0917a6ef58f87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lines: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lang_code = 'ru'\n",
    "\n",
    "\n",
    "word_vectors,_ = load_vectors(VECTORS.format(size=SIZE, lang_code=lang_code, dim=DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:18:54.728320Z",
     "start_time": "2021-05-02T14:18:54.709962Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "RECURRENT = True\n",
    "DENSE_UNITS = [128,]\n",
    "RNN_HIDDEN = 128\n",
    "NUM_LAYERS = 1\n",
    "BIDIRECT = False\n",
    "RNN_DROPOUT = 0.\n",
    "\n",
    "ETA = 0.01\n",
    "DROPOUT = 0.25\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T14:18:56.669074Z",
     "start_time": "2021-05-02T14:18:54.730120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopicClassifier(\n",
      "  (embedding_net): EmbeddingNet()\n",
      "  (output_net): RecurrentNet(\n",
      "    (recurrent_layer): GRU(300, 128, batch_first=True)\n",
      "    (clf): Linear(in_features=128, out_features=53, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_loader,test_loader = create_loaders(data, lang_code, classes=CLASSES, batch_size=BATCH_SIZE)\n",
    "\n",
    "embed = EmbeddingNet(word_vectors, DIM)\n",
    "\n",
    "if RECURRENT:\n",
    "    dense = RecurrentNet(DIM, len(CLASSES), RNN_HIDDEN, NUM_LAYERS, BIDIRECT, RNN_DROPOUT)\n",
    "else:\n",
    "    dense = DenseNet(DIM * 4, DENSE_UNITS, len(CLASSES), DROPOUT)\n",
    "\n",
    "clf = TopicClassifier(embed, dense, is_rnn=RECURRENT)\n",
    "\n",
    "print(clf)\n",
    "\n",
    "criterion = nn.L1Loss(reduction='sum')\n",
    "\n",
    "optimizer = optim.Adam(dense.parameters(), lr=ETA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T15:25:37.197705Z",
     "start_time": "2021-05-02T15:21:10.346914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train 1.0862, eval 1.1175\n",
      "Epoch 2: train 1.0949, eval 1.1315\n",
      "Epoch 3: train 1.0948, eval 1.1181\n",
      "Epoch 4: train 1.0861, eval 1.1084\n",
      "Epoch 5: train 1.0896, eval 1.1460\n",
      "Epoch 6: train 1.1039, eval 1.1501\n",
      "Epoch 7: train 1.1036, eval 1.1539\n",
      "Epoch 8: train 1.1003, eval 1.1306\n",
      "Epoch 9: train 1.0867, eval 1.1331\n",
      "Epoch 10: train 1.0913, eval 1.1136\n",
      "Scores: train 0.48, eval 0.47\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCHS):\n",
    "    # train\n",
    "    clf.train()\n",
    "    train_loss = 0.\n",
    "    train_size = 0.\n",
    "    for x,y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = clf(x)\n",
    "        loss = criterion(out, y)\n",
    "        train_loss += loss.item()\n",
    "        train_size += y.size(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= train_size\n",
    "    # eval\n",
    "    clf.eval()\n",
    "    test_loss = 0.\n",
    "    test_size = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in test_loader:\n",
    "            out = clf(x)\n",
    "            loss = criterion(out, y)\n",
    "            test_loss += loss.item()\n",
    "            test_size += y.size(0)\n",
    "    test_loss /= test_size\n",
    "    print(f\"Epoch {i+1}: train {train_loss:.4f}, eval {test_loss:.4f}\")\n",
    "    \n",
    "print(f\"Scores: train {loss2score(train_loss):.2f}, eval {loss2score(test_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgcat",
   "language": "python",
   "name": "tgcat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "notify_time": "10",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "217px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
